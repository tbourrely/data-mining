{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitvenvvenv65f5fc2ba572422d87050d0a13b08264",
   "display_name": "Python 3.7.6 64-bit ('venv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([(b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b't', b'?', b't', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b't', b't', b't', b't', b'?', b't', b'?', b't', b't', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'high'),\n       (b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b't', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b't', b't', b'?', b'?', b'?', b'?', b'?', b't', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'low'),\n       (b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b'?', b't', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'low'),\n       ...,\n       (b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b't', b't', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b't', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'low'),\n       (b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b't', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'low'),\n       (b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b't', b't', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b't', b't', b't', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b't', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b't', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'?', b'high')],\n      dtype=[('department1', 'S1'), ('department2', 'S1'), ('department3', 'S1'), ('department4', 'S1'), ('department5', 'S1'), ('department6', 'S1'), ('department7', 'S1'), ('department8', 'S1'), ('department9', 'S1'), ('grocery misc', 'S1'), ('department11', 'S1'), ('baby needs', 'S1'), ('bread and cake', 'S1'), ('baking needs', 'S1'), ('coupons', 'S1'), ('juice-sat-cord-ms', 'S1'), ('tea', 'S1'), ('biscuits', 'S1'), ('canned fish-meat', 'S1'), ('canned fruit', 'S1'), ('canned vegetables', 'S1'), ('breakfast food', 'S1'), ('cigs-tobacco pkts', 'S1'), ('cigarette cartons', 'S1'), ('cleaners-polishers', 'S1'), ('coffee', 'S1'), ('sauces-gravy-pkle', 'S1'), ('confectionary', 'S1'), ('puddings-deserts', 'S1'), ('dishcloths-scour', 'S1'), ('deod-disinfectant', 'S1'), ('frozen foods', 'S1'), ('razor blades', 'S1'), ('fuels-garden aids', 'S1'), ('spices', 'S1'), ('jams-spreads', 'S1'), ('insecticides', 'S1'), ('pet foods', 'S1'), ('laundry needs', 'S1'), ('party snack foods', 'S1'), ('tissues-paper prd', 'S1'), ('wrapping', 'S1'), ('dried vegetables', 'S1'), ('pkt-canned soup', 'S1'), ('soft drinks', 'S1'), ('health food other', 'S1'), ('beverages hot', 'S1'), ('health&beauty misc', 'S1'), ('deodorants-soap', 'S1'), ('menstoiletries', 'S1'), ('medicines', 'S1'), ('haircare', 'S1'), ('dental needs', 'S1'), ('lotions-creams', 'S1'), ('sanitary pads', 'S1'), ('cough-cold-pain', 'S1'), ('department57', 'S1'), ('meat misc', 'S1'), ('cheese', 'S1'), ('chickens', 'S1'), ('milk-cream', 'S1'), ('cold-meats', 'S1'), ('deli gourmet', 'S1'), ('margarine', 'S1'), ('salads', 'S1'), ('small goods', 'S1'), ('dairy foods', 'S1'), ('fruit drinks', 'S1'), ('delicatessen misc', 'S1'), ('department70', 'S1'), ('beef', 'S1'), ('hogget', 'S1'), ('lamb', 'S1'), ('pet food', 'S1'), ('pork', 'S1'), ('poultry', 'S1'), ('veal', 'S1'), ('gourmet meat', 'S1'), ('department79', 'S1'), ('department80', 'S1'), ('department81', 'S1'), ('produce misc', 'S1'), ('fruit', 'S1'), ('plants', 'S1'), ('potatoes', 'S1'), ('vegetables', 'S1'), ('flowers', 'S1'), ('department88', 'S1'), ('department89', 'S1'), ('variety misc', 'S1'), ('brushware', 'S1'), ('electrical', 'S1'), ('haberdashery', 'S1'), ('kitchen', 'S1'), ('manchester', 'S1'), ('pantyhose', 'S1'), ('plasticware', 'S1'), ('department98', 'S1'), ('stationary', 'S1'), ('department100', 'S1'), ('department101', 'S1'), ('department102', 'S1'), ('prepared meals', 'S1'), ('preserving needs', 'S1'), ('condiments', 'S1'), ('cooking oils', 'S1'), ('department107', 'S1'), ('department108', 'S1'), ('department109', 'S1'), ('department110', 'S1'), ('department111', 'S1'), ('department112', 'S1'), ('department113', 'S1'), ('department114', 'S1'), ('health food bulk', 'S1'), ('department116', 'S1'), ('department117', 'S1'), ('department118', 'S1'), ('department119', 'S1'), ('department120', 'S1'), ('bake off products', 'S1'), ('department122', 'S1'), ('department123', 'S1'), ('department124', 'S1'), ('department125', 'S1'), ('department126', 'S1'), ('department127', 'S1'), ('department128', 'S1'), ('department129', 'S1'), ('department130', 'S1'), ('small goods2', 'S1'), ('offal', 'S1'), ('mutton', 'S1'), ('trim pork', 'S1'), ('trim lamb', 'S1'), ('imported cheese', 'S1'), ('department137', 'S1'), ('department138', 'S1'), ('department139', 'S1'), ('department140', 'S1'), ('department141', 'S1'), ('department142', 'S1'), ('department143', 'S1'), ('department144', 'S1'), ('department145', 'S1'), ('department146', 'S1'), ('department147', 'S1'), ('department148', 'S1'), ('department149', 'S1'), ('department150', 'S1'), ('department151', 'S1'), ('department152', 'S1'), ('department153', 'S1'), ('department154', 'S1'), ('department155', 'S1'), ('department156', 'S1'), ('department157', 'S1'), ('department158', 'S1'), ('department159', 'S1'), ('department160', 'S1'), ('department161', 'S1'), ('department162', 'S1'), ('department163', 'S1'), ('department164', 'S1'), ('department165', 'S1'), ('department166', 'S1'), ('department167', 'S1'), ('department168', 'S1'), ('department169', 'S1'), ('department170', 'S1'), ('department171', 'S1'), ('department172', 'S1'), ('department173', 'S1'), ('department174', 'S1'), ('department175', 'S1'), ('department176', 'S1'), ('department177', 'S1'), ('department178', 'S1'), ('department179', 'S1'), ('casks white wine', 'S1'), ('casks red wine', 'S1'), ('750ml white nz', 'S1'), ('750ml red nz', 'S1'), ('750ml white imp', 'S1'), ('750ml red imp', 'S1'), ('sparkling nz', 'S1'), ('sparkling imp', 'S1'), ('brew kits/accesry', 'S1'), ('department189', 'S1'), ('port and sherry', 'S1'), ('ctrled label wine', 'S1'), ('department192', 'S1'), ('department193', 'S1'), ('department194', 'S1'), ('department195', 'S1'), ('department196', 'S1'), ('department197', 'S1'), ('department198', 'S1'), ('department199', 'S1'), ('non host support', 'S1'), ('department201', 'S1'), ('department202', 'S1'), ('department203', 'S1'), ('department204', 'S1'), ('department205', 'S1'), ('department206', 'S1'), ('department207', 'S1'), ('department208', 'S1'), ('department209', 'S1'), ('department210', 'S1'), ('department211', 'S1'), ('department212', 'S1'), ('department213', 'S1'), ('department214', 'S1'), ('department215', 'S1'), ('department216', 'S1'), ('total', 'S4')])"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "data, meta = arff.loadarff('./data/supermarket.arff')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_one_hot = pd.get_dummies(pd.DataFrame(data))\n",
    "# find cols with interrogation marks\n",
    "cols_with_interrogation_mark = supermarket_one_hot.filter(regex='\\'\\?\\'$',axis=1).columns # Fix regex as it did not match any cols on my environment...\n",
    "# delete columsn with interrogation marks in it\n",
    "supermarket_one_hot.drop(cols_with_interrogation_mark,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "apriori_result n of rows : 10282\n"
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "apriori_result = apriori(supermarket_one_hot, min_support=0.1)\n",
    "print('apriori_result n of rows : {}'.format(apriori_result.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a bar chart, itemsets frequencies in terms of items containes\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_itemsets_len = apriori_result.itemsets.map(len).max()\n",
    "frequencies = {}\n",
    "\n",
    "for i in range(1, max_itemsets_len + 1):\n",
    "   frequencies[str(i)] = apriori_result.loc[map(lambda x: len(x) == i, apriori_result['itemsets'])].shape[0]\n",
    "\n",
    "x = np.arange(len(frequencies.keys()))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects = ax.bar(x - width/2, frequencies.values(), width, label='itemsets')\n",
    "\n",
    "# Add value to each bar\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Items contained')\n",
    "ax.set_title('itemsets frequencies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(frequencies.keys())\n",
    "ax.legend()\n",
    "plt.savefig('fig/itemsets_frequencies')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of association rules : 24570\n-------------\nFirst association rule : \nantecedents                  (0)\nconsequents                 (11)\nantecedent support      0.226281\nconsequent support      0.719689\nsupport                 0.171601\nconfidence              0.758357\nlift                     1.05373\nleverage              0.00874991\nconviction               1.16002\nName: 0, dtype: object\n"
    }
   ],
   "source": [
    "# build association rules\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "min_threshold = 0.7\n",
    "dataset_association_rules = association_rules(apriori_result, min_threshold=min_threshold)\n",
    "print('Number of association rules : {}'.format(dataset_association_rules.shape[0]))\n",
    "print('-------------')\n",
    "print('First association rule : ')\n",
    "print(dataset_association_rules.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of rules with 5 items : 8811\n"
    }
   ],
   "source": [
    "# rules with 5 items (4 antecedents, 1 consequent)\n",
    "\n",
    "# select those with 4 antecedents\n",
    "rules_with_5_items = dataset_association_rules.loc[map(lambda x: len(x) == 4, dataset_association_rules['antecedents'])]\n",
    "rules_with_5_items = rules_with_5_items.loc[map(lambda x: len(x) == 1, rules_with_5_items['consequents'])]\n",
    "\n",
    "print('Number of rules with 5 items : {}'.format(rules_with_5_items.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antecedent support</th>\n      <th>consequent support</th>\n      <th>support</th>\n      <th>confidence</th>\n      <th>lift</th>\n      <th>leverage</th>\n      <th>conviction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n      <td>8811.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.148910</td>\n      <td>0.603212</td>\n      <td>0.117313</td>\n      <td>0.791140</td>\n      <td>1.332255</td>\n      <td>0.027822</td>\n      <td>1.978909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022173</td>\n      <td>0.082035</td>\n      <td>0.015988</td>\n      <td>0.054682</td>\n      <td>0.177603</td>\n      <td>0.009443</td>\n      <td>0.413518</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.108494</td>\n      <td>0.362870</td>\n      <td>0.100065</td>\n      <td>0.700000</td>\n      <td>1.062178</td>\n      <td>0.006364</td>\n      <td>1.189966</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.133348</td>\n      <td>0.563000</td>\n      <td>0.105252</td>\n      <td>0.747244</td>\n      <td>1.220572</td>\n      <td>0.020979</td>\n      <td>1.675085</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.144586</td>\n      <td>0.604063</td>\n      <td>0.112816</td>\n      <td>0.783489</td>\n      <td>1.290764</td>\n      <td>0.026576</td>\n      <td>1.930478</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.159282</td>\n      <td>0.640156</td>\n      <td>0.124919</td>\n      <td>0.829582</td>\n      <td>1.388577</td>\n      <td>0.032749</td>\n      <td>2.231455</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.283337</td>\n      <td>0.719689</td>\n      <td>0.202939</td>\n      <td>0.929795</td>\n      <td>2.199346</td>\n      <td>0.072145</td>\n      <td>4.517902</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       antecedent support  consequent support      support   confidence  \\\ncount         8811.000000         8811.000000  8811.000000  8811.000000   \nmean             0.148910            0.603212     0.117313     0.791140   \nstd              0.022173            0.082035     0.015988     0.054682   \nmin              0.108494            0.362870     0.100065     0.700000   \n25%              0.133348            0.563000     0.105252     0.747244   \n50%              0.144586            0.604063     0.112816     0.783489   \n75%              0.159282            0.640156     0.124919     0.829582   \nmax              0.283337            0.719689     0.202939     0.929795   \n\n              lift     leverage   conviction  \ncount  8811.000000  8811.000000  8811.000000  \nmean      1.332255     0.027822     1.978909  \nstd       0.177603     0.009443     0.413518  \nmin       1.062178     0.006364     1.189966  \n25%       1.220572     0.020979     1.675085  \n50%       1.290764     0.026576     1.930478  \n75%       1.388577     0.032749     2.231455  \nmax       2.199346     0.072145     4.517902  "
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_with_5_items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------------------------------\nBest association rule for metric confidence\n-----------------------------------------------------\nantecedents           (37, 77, 15, 80, 122, 29)\nconsequents                                (11)\nantecedent support                     0.110223\nconsequent support                     0.719689\nsupport                                0.103307\nconfidence                             0.937255\nlift                                    1.30231\nleverage                              0.0239807\nconviction                              4.46746\nName: 24524, dtype: object\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nBest association rule for metric lift\n-----------------------------------------------------\nantecedents           (38, 77, 15, 80, 29)\nconsequents                      (122, 11)\nantecedent support                0.143289\nconsequent support                0.305381\nsupport                           0.102658\nconfidence                         0.71644\nlift                               2.34605\nleverage                         0.0589004\nconviction                         2.44964\nName: 24549, dtype: object\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nBest association rule for metric leverage\n-----------------------------------------------------\nantecedents           (24, 29, 38)\nconsequents                  (122)\nantecedent support        0.201643\nconsequent support         0.36287\nsupport                   0.146315\nconfidence                0.725616\nlift                       1.99966\nleverage                 0.0731451\nconviction                 2.32204\nName: 9512, dtype: object\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nBest association rule for metric conviction\n-----------------------------------------------------\nantecedents           (66, 12, 77, 122)\nconsequents                        (80)\nantecedent support             0.113897\nconsequent support             0.639939\nsupport                         0.10482\nconfidence                     0.920304\nlift                            1.43811\nleverage                      0.0319325\nconviction                       4.5179\nName: 18332, dtype: object\n--------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# find best rules for each metrics\n",
    "metrics = ('confidence', 'lift', 'leverage', 'conviction')\n",
    "\n",
    "for metric in metrics:\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print('Best association rule for metric {}'.format(metric))\n",
    "    print('-----------------------------------------------------')\n",
    "    print(dataset_association_rules.iloc[dataset_association_rules[metric].idxmax()])\n",
    "    print('--------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean columns names\n",
    "import re\n",
    "supermarket_one_hot.rename(columns=lambda x: re.sub('_b\\'t\\'$', '', x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['bread and cake', 'baking needs', 'biscuits', 'frozen foods', 'margarine', 'fruit', 'vegetables']\n"
    }
   ],
   "source": [
    "# search for frequent items bought with coffee and milk-cream\n",
    "positioning_apriori_result = apriori(supermarket_one_hot, min_support=0.1, use_colnames=True)\n",
    "itemsets_with_coffee_and_milkcream = positioning_apriori_result.loc[map(lambda x: 'milk-cream' in x and 'coffee' in x, positioning_apriori_result['itemsets'])]\n",
    "\n",
    "frequently_bought_items = []\n",
    "excluded = ('coffee', 'milk-cream')\n",
    "for item in itemsets_with_coffee_and_milkcream['itemsets']:\n",
    "    not_coffee_nor_milkcream = [element for element in item if element not in excluded]\n",
    "\n",
    "    if len(not_coffee_nor_milkcream):\n",
    "        frequently_bought_items.extend(not_coffee_nor_milkcream)\n",
    "\n",
    "\n",
    "frequently_bought_items = list(dict.fromkeys(frequently_bought_items)) # remove duplicates\n",
    "\n",
    "print(frequently_bought_items)"
   ]
  }
 ]
}